Questions:
- What is the advantage of having more nodes?
- Why do activation functions have the shape that they do?
- What determines the shape of an activation function?

Learning
- Deep:
- Machine: Machine learning refers to any computational algorithm that performs a specific task without explicit 
instruction. Such an algorithm relies on pattern recognition and inference in order to complete the task and 
learns by experience (taking in many repeated examples of the data to be identified, classified, etc). In this 
way, machine learning attempts to mimic the way that humans learn. As a child, you learn that a spoon is called 
a "spoon" because a parent repeatedly refers to it as such. Children may misclassify their experiences because 
they have few reference points from which to assess them. Adults are typically more skilled at processing and 
classifying their experiences because they have a foundation of pattern recognition on which to base their 
analysis. Machine learning is an important technique among tasks for which explicit instructions are impossible
to provide (i.e. "tell me which of these objects humans have never seen before").
Transfer:

Neural Networks, Types
- Artificial: Otherwise known as simply a "neural network", ANNs are a form of machine learning algorithm whose 
architecture is made up of interconnected nodes or "neurons". Neurons receive, process, and transmit information 
among one another, collectively transforming input to output.
- Bayesian:
- Convolutional:
- Deep:
- Fully Connected Feedforward:
- Long Short Term Memory:
- Recurrent:

Neural Networks, Anatomy
- Activation Function: Otherwise known as a "transfer function", activation functions determine the output of a neuron. 
An activation function is a mathematical proxy to the rate of action potential within a cell. In its most simplistic 
form, a neuron has two possible outputs: "off" - 0, and "on" - 1. With more complex outputs (i.e. a function that 
interpolates between 0 and 1), one may construct a neural network capable of more complex tasks. In order to learn 
non-trivial tasks, a neural network must contain nonlinear activation functions. Activation functions typically have a 
sigmoid shape and thus are represented with a "sigma" symbol. "They are also often monotonically increasing, 
continuous, differentiable and bounded."
- Gates:
- Layers: The aggregation of a number of 
- Neuron: A mathematical function that receives one or more inputs, creates a linear combination of the inputs 
(sums them with weights), passes the linear combination through a non-linear function ("activation function"), and 
transmits the activation function resulting value as an output. This output may undergo further processing by 
other layers within the neural network.
- Perceptron: 
- Thresholds:
- Weights:


Miscellaneous

- Architecture:
- Data Mining:
- Gradient:
- Heaviside: 
- Logic Gates:
- Training Set: The set of example information from which a machine learning algorithm learns its intended task. 
Constructing a "good" (comprehensive, reprepresentative) training set is important, because it determines the 
degree to which your algorithm will perform its intended task in the future. If you want a child to speak 
multiple languages, they must receive input from native speakers of those languages. Similarly, a machine
learning algorithm can only reproduce what it has been shown.
- Testing Set: 

